{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d448a2",
   "metadata": {},
   "source": [
    "# Project Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f03c82",
   "metadata": {},
   "source": [
    "Note: Here is the step by step of building the model.\n",
    "\n",
    "\n",
    "DISCLAIMER: STILL WORK ON PROGRESS - VERY MESSY CODE. THANKS\n",
    "\n",
    "__Establishing Dataset__\n",
    "\n",
    "Dataset source\n",
    "\n",
    "    https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
    "\n",
    "    https://data.mendeley.com/datasets/f45bkkt8pr/1\n",
    "    \n",
    "    from Jake Currie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e607fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirements\n",
    "import re\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a1b6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset importing\n",
    "\n",
    "#Importing - the dir is my local files\n",
    "\n",
    "\n",
    "dataset1_dir = r\"C:\\Users\\user\\GIT Project\\Prototype_Halim\\Model Creation Protocol\\Datasets\\spam.csv\"\n",
    "dataset2_dir = r\"C:\\Users\\user\\GIT Project\\Prototype_Halim\\Model Creation Protocol\\Datasets\\Dataset_5971.csv\"\n",
    "dataset3_dir = r\"C:\\Users\\user\\GIT Project\\Prototype_Halim\\Model Creation Protocol\\Datasets\\Spam_SMS_Final.csv\"\n",
    "\n",
    "#Merging 1 and 2\n",
    "dataset1 = pd.read_csv(dataset1_dir)\n",
    "dataset1_selected = dataset1.iloc[:,:2]\n",
    "dataset1_selected.columns = ['Label','Text']\n",
    "\n",
    "dataset2 = pd.read_csv(dataset2_dir)\n",
    "dataset2_selected = dataset2.iloc[:,:2]\n",
    "dataset2_selected.columns = ['Label','Text']\n",
    "\n",
    "\n",
    "dataset = pd.concat([dataset1_selected,dataset2_selected])\n",
    "\n",
    "#Merging 1&2 and 3\n",
    "dataset3 = pd.read_csv(dataset2_dir)\n",
    "dataset3_selected = dataset2.iloc[:,:2]\n",
    "dataset3_selected.columns = ['Label','Text']\n",
    "\n",
    "dataset = pd.concat([dataset,dataset3_selected])\n",
    "\n",
    "#replace the smishing value to be spam\n",
    "dataset[\"Label\"] = dataset[\"Label\"].replace(['Smishing','Spam','smishing'], 'spam')\n",
    "\n",
    "\n",
    "#since its collection of data, it sure have duplication so here delete the duplicates\n",
    "dataset = dataset.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c993b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing text\n",
    "dataset[\"Text\"] = dataset[\"Text\"].str.lower() #make every word lower\n",
    "dataset['Text'] = dataset['Text'].str.replace('[^\\w\\s]', '', regex=True) #remove non alphanumeric char in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96a95662",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting\n",
    "X = dataset[\"Text\"]\n",
    "y = dataset[\"Label\"]\n",
    "\n",
    "#Split 80:20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3087609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define extractor\n",
    "vectorizers = {CountVectorizer(), \n",
    "             TfidfVectorizer()}\n",
    "\n",
    "#Define Machine Learning\n",
    "classifiers = {MultinomialNB(),\n",
    "              SVC(probability=True),\n",
    "              DecisionTreeClassifier(),\n",
    "              MLPClassifier()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a05e128",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION OF  MLPClassifier() TfidfVectorizer() \n",
      "\n",
      "Accuracy: 0.9875183553597651\n",
      "Confusion Matrix:\n",
      "[[1082    6]\n",
      " [  11  263]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      1088\n",
      "        spam       0.98      0.96      0.97       274\n",
      "\n",
      "    accuracy                           0.99      1362\n",
      "   macro avg       0.98      0.98      0.98      1362\n",
      "weighted avg       0.99      0.99      0.99      1362\n",
      "\n",
      "\n",
      "################################################################\n",
      "\n",
      "EVALUATION OF  SVC(probability=True) TfidfVectorizer() \n",
      "\n",
      "Accuracy: 0.9838472834067548\n",
      "Confusion Matrix:\n",
      "[[1086    2]\n",
      " [  20  254]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1088\n",
      "        spam       0.99      0.93      0.96       274\n",
      "\n",
      "    accuracy                           0.98      1362\n",
      "   macro avg       0.99      0.96      0.97      1362\n",
      "weighted avg       0.98      0.98      0.98      1362\n",
      "\n",
      "\n",
      "################################################################\n",
      "\n",
      "EVALUATION OF  MultinomialNB() TfidfVectorizer() \n",
      "\n",
      "Accuracy: 0.9669603524229075\n",
      "Confusion Matrix:\n",
      "[[1088    0]\n",
      " [  45  229]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1088\n",
      "        spam       1.00      0.84      0.91       274\n",
      "\n",
      "    accuracy                           0.97      1362\n",
      "   macro avg       0.98      0.92      0.95      1362\n",
      "weighted avg       0.97      0.97      0.97      1362\n",
      "\n",
      "\n",
      "################################################################\n",
      "\n",
      "EVALUATION OF  DecisionTreeClassifier() TfidfVectorizer() \n",
      "\n",
      "Accuracy: 0.9618208516886931\n",
      "Confusion Matrix:\n",
      "[[1075   13]\n",
      " [  39  235]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      0.99      0.98      1088\n",
      "        spam       0.95      0.86      0.90       274\n",
      "\n",
      "    accuracy                           0.96      1362\n",
      "   macro avg       0.96      0.92      0.94      1362\n",
      "weighted avg       0.96      0.96      0.96      1362\n",
      "\n",
      "\n",
      "################################################################\n",
      "\n",
      "EVALUATION OF  MLPClassifier() CountVectorizer() \n",
      "\n",
      "Accuracy: 0.9853157121879589\n",
      "Confusion Matrix:\n",
      "[[1085    3]\n",
      " [  17  257]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1088\n",
      "        spam       0.99      0.94      0.96       274\n",
      "\n",
      "    accuracy                           0.99      1362\n",
      "   macro avg       0.99      0.97      0.98      1362\n",
      "weighted avg       0.99      0.99      0.99      1362\n",
      "\n",
      "\n",
      "################################################################\n",
      "\n",
      "EVALUATION OF  SVC(probability=True) CountVectorizer() \n",
      "\n",
      "Accuracy: 0.9779735682819384\n",
      "Confusion Matrix:\n",
      "[[1084    4]\n",
      " [  26  248]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1088\n",
      "        spam       0.98      0.91      0.94       274\n",
      "\n",
      "    accuracy                           0.98      1362\n",
      "   macro avg       0.98      0.95      0.96      1362\n",
      "weighted avg       0.98      0.98      0.98      1362\n",
      "\n",
      "\n",
      "################################################################\n",
      "\n",
      "EVALUATION OF  MultinomialNB() CountVectorizer() \n",
      "\n",
      "Accuracy: 0.9838472834067548\n",
      "Confusion Matrix:\n",
      "[[1082    6]\n",
      " [  16  258]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      1088\n",
      "        spam       0.98      0.94      0.96       274\n",
      "\n",
      "    accuracy                           0.98      1362\n",
      "   macro avg       0.98      0.97      0.97      1362\n",
      "weighted avg       0.98      0.98      0.98      1362\n",
      "\n",
      "\n",
      "################################################################\n",
      "\n",
      "EVALUATION OF  DecisionTreeClassifier() CountVectorizer() \n",
      "\n",
      "Accuracy: 0.9669603524229075\n",
      "Confusion Matrix:\n",
      "[[1074   14]\n",
      " [  31  243]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      0.99      0.98      1088\n",
      "        spam       0.95      0.89      0.92       274\n",
      "\n",
      "    accuracy                           0.97      1362\n",
      "   macro avg       0.96      0.94      0.95      1362\n",
      "weighted avg       0.97      0.97      0.97      1362\n",
      "\n",
      "\n",
      "################################################################\n",
      "\n",
      "\n",
      "\n",
      "In summary, the highest performance is  MLPClassifier() TfidfVectorizer()  :  0.9875183553597651\n"
     ]
    }
   ],
   "source": [
    "#Training multiple classifier\n",
    "\n",
    "#Prepare to save the best performance\n",
    "highest_acc = 0\n",
    "highest_model = \"\"\n",
    "highest_ext = \"\"\n",
    "\n",
    "#training and testing begin.\n",
    "for vectorizer in vectorizers: #iterate each extractor\n",
    "    #vectorized the data\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "    \n",
    "    for model in classifiers: #iterate each machine learning\n",
    "        print(\"EVALUATION OF \", model, vectorizer,\"\\n\")\n",
    "        \n",
    "        #training\n",
    "        model.fit(X_train_vectorized, y_train)\n",
    "        \n",
    "        #testing\n",
    "        y_pred = model.predict(X_test_vectorized)\n",
    "        clf_report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        #calculate performance\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        confusion = confusion_matrix(y_test, y_pred)\n",
    "        classification_rep = classification_report(y_test, y_pred)\n",
    "        \n",
    "        #show performance results\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print(f'Confusion Matrix:\\n{confusion}')\n",
    "        print(f'Classification Report:\\n{classification_rep}')\n",
    "        \n",
    "        #saving the best performance.\n",
    "        if(accuracy >= highest_acc):\n",
    "            highest_acc = accuracy\n",
    "            highest_model = model\n",
    "            highest_ext = vectorizer\n",
    "        print(\"\\n################################################################\\n\")\n",
    "\n",
    "#print the best performance           \n",
    "print(\"\\n\\nIn summary, the highest performance is \",highest_model,highest_ext,\" : \",highest_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774c48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13c7a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model concludes this\n",
      "### Congratulation You won A CAR, please call 08707509020 to take your CAR ###\n",
      " is  ham  with 83.30% confident. \n",
      "\n",
      "The model concludes this\n",
      "### Records indicate you were involved in an accident in the last 2.5 years. As such you are entitled to claim compensation, Reply CLAIM for info or STOP to opt out ###\n",
      " is  spam  with 100.00% confident. \n",
      "\n",
      "The model concludes this\n",
      "### call now 08707509020 to claim your prize ###\n",
      " is  spam  with 98.77% confident. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing ground\n",
    "#take best extractor\n",
    "best_vectorizer = highest_ext\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    \n",
    "#take best model\n",
    "best_model = highest_model\n",
    "best_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "#New Message\n",
    "new_text = [\"Congratulation You won A CAR, please call 08707509020 to take your CAR\",\n",
    "            \"Records indicate you were involved in an accident in the last 2.5 years. As such you are entitled to claim compensation, Reply CLAIM for info or STOP to opt out\",\n",
    "            \"call now 08707509020 to claim your prize\"]\n",
    "\n",
    "#pre process the new Message \n",
    "preprocessed_text = [s.lower() for s in new_text]\n",
    "preprocessed_text = [re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower() for text in new_text]\n",
    "\n",
    "#predicting it is smishing or not\n",
    "new_text_vectorized = vectorizer.transform(new_text)\n",
    "y_pred = best_model.predict(new_text_vectorized)\n",
    "y_prob = best_model.predict_proba(new_text_vectorized)\n",
    "\n",
    "#print the results\n",
    "iteration = 0\n",
    "for i in y_pred:\n",
    "    number = max(y_prob[iteration])\n",
    "    print(\"The model concludes this\\n###\",new_text[iteration],\"###\\n is \", i, \" with {:.2f}% confident. \\n\".format(number*100))\n",
    "    iteration = iteration + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018783b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model and the extractor\n",
    "#save the model\n",
    "model_filename = \"Smishing_Model.h5\"\n",
    "\n",
    "joblib.dump(best_model, model_filename)\n",
    "print(\"Model is saved successfully!\")\n",
    "\n",
    "#save the extractor\n",
    "tfid_matrix = best_vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "#save extractor as pickle\n",
    "with open(\"Smishing_Extractor.pickle\",\"wb\") as file:\n",
    "    pickle.dump(tfid_matrix, file)\n",
    "\n",
    "\n",
    "\n",
    "#save extractor as json\n",
    "\n",
    "import json\n",
    "\n",
    "array_matrix = tfid_matrix.toarray()\n",
    "json_matrix = array_matrix.tolist()\n",
    "\n",
    "with open(\"Smishing_Extractor.json\", \"w\") as json_file:\n",
    "    json.dump(json_matrix, json_file)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Extractor is saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36618dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5446x9600 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 74841 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96951e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3ae76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c99c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
